{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    \"\"\"\n",
    "    创建数据集\n",
    "    :return: 单词列表postingList, 所属类别classVec\n",
    "    \"\"\"\n",
    "    postingList = [['my', 'my', 'has', 'flea', 'problems', 'help', 'please'], #[0,0,1,1,1......]\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]  # 1 is abusive, 0 not\n",
    "    return postingList, classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    \"\"\"\n",
    "    获取所有单词的集合\n",
    "    :param dataSet: 数据集\n",
    "    :return: 所有单词的集合(即不含重复元素的单词列表)\n",
    "    \"\"\"\n",
    "    vocabSet = set([])  # create empty set\n",
    "    for document in dataSet:\n",
    "        # 操作符 | 用于求两个集合的并集\n",
    "        vocabSet = vocabSet | set(document)  # union of the two sets\n",
    "    return list(vocabSet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    \"\"\"\n",
    "    遍历查看该单词是否出现，出现该单词则将该单词置1\n",
    "    :param vocabList: 所有单词集合列表\n",
    "    :param inputSet: 输入数据集\n",
    "    :return: 匹配列表[0,1,0,1...]，其中 1与0 表示词汇表中的单词是否出现在输入的数据集中\n",
    "    \"\"\"\n",
    "    # 创建一个和词汇表等长的向量，并将其元素都设置为0\n",
    "    returnVec = [0] * len(vocabList)# [0,0......]\n",
    "    # 遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _trainNB0(trainMatrix, trainCategory):\n",
    "    \"\"\"\n",
    "    训练数据原版\n",
    "    :param trainMatrix: 文件单词矩阵 [[1,0,1,1,1....],[],[]...]\n",
    "    :param trainCategory: 文件对应的类别[0,1,1,0....]，列表长度等于单词矩阵数，其中的1代表对应的文件是侮辱性文件，0代表不是侮辱性矩阵\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 文件数\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    # 单词数\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # 侮辱性文件的出现概率，即trainCategory中所有的1的个数，\n",
    "    # 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    # 构造单词出现次数列表\n",
    "    p0Num = zeros(numWords) # [0,0,0,.....]\n",
    "    p1Num = zeros(numWords) # [0,0,0,.....]\n",
    "\n",
    "    # 整个数据集单词出现总数\n",
    "    p0Denom = 0.0\n",
    "    p1Denom = 0.0\n",
    "    for i in range(numTrainDocs):\n",
    "        # 是否是侮辱性文件\n",
    "        if trainCategory[i] == 1:\n",
    "            # 如果是侮辱性文件，对侮辱性文件的向量进行加和\n",
    "            p1Num += trainMatrix[i] #[0,1,1,....] + [0,1,1,....]->[0,2,2,...]\n",
    "            # 对向量中的所有元素进行求和，也就是计算所有侮辱性文件中出现的单词总数\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # 类别1，即侮辱性文档的[P(F1|C1),P(F2|C1),P(F3|C1),P(F4|C1),P(F5|C1)....]列表\n",
    "    # 即 在1类别下，每个单词出现的概率\n",
    "    p1Vect = p1Num / p1Denom# [1,2,3,5]/90->[1/90,...]\n",
    "    # 类别0，即正常文档的[P(F1|C0),P(F2|C0),P(F3|C0),P(F4|C0),P(F5|C0)....]列表\n",
    "    # 即 在0类别下，每个单词出现的概率\n",
    "    p0Vect = p0Num / p0Denom\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOPosts, listClasses = loadDataSet()\n",
    "myVocabList =createVocabList(listOPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
       " ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
       " ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
       " ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
       " ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
       " ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOPosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dalmation',\n",
       " 'love',\n",
       " 'posting',\n",
       " 'help',\n",
       " 'mr',\n",
       " 'maybe',\n",
       " 'worthless',\n",
       " 'please',\n",
       " 'my',\n",
       " 'buying',\n",
       " 'dog',\n",
       " 'cute',\n",
       " 'quit',\n",
       " 'so',\n",
       " 'ate',\n",
       " 'park',\n",
       " 'not',\n",
       " 'stupid',\n",
       " 'problems',\n",
       " 'take',\n",
       " 'has',\n",
       " 'how',\n",
       " 'I',\n",
       " 'garbage',\n",
       " 'to',\n",
       " 'licks',\n",
       " 'food',\n",
       " 'flea',\n",
       " 'steak',\n",
       " 'stop',\n",
       " 'is',\n",
       " 'him']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = []\n",
    "for post_in in listOPosts:\n",
    "    train_mat.append(setOfWords2Vec(myVocabList, post_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 1, 1, 0, 0, 1, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pov,p1v,pAb=_trainNB0(train_mat, listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04166667, 0.04166667, 0.        , 0.04166667, 0.04166667,\n",
       "       0.        , 0.        , 0.04166667, 0.125     , 0.        ,\n",
       "       0.04166667, 0.04166667, 0.        , 0.04166667, 0.04166667,\n",
       "       0.        , 0.        , 0.        , 0.04166667, 0.        ,\n",
       "       0.04166667, 0.04166667, 0.04166667, 0.        , 0.04166667,\n",
       "       0.04166667, 0.        , 0.04166667, 0.04166667, 0.04166667,\n",
       "       0.04166667, 0.08333333])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.05263158, 0.        , 0.        ,\n",
       "       0.05263158, 0.10526316, 0.        , 0.        , 0.05263158,\n",
       "       0.10526316, 0.        , 0.05263158, 0.        , 0.        ,\n",
       "       0.05263158, 0.05263158, 0.15789474, 0.        , 0.05263158,\n",
       "       0.        , 0.        , 0.        , 0.05263158, 0.05263158,\n",
       "       0.        , 0.05263158, 0.        , 0.        , 0.05263158,\n",
       "       0.        , 0.05263158])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB(train_mat, train_category):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    朴素贝叶斯分类修正版，　注意和原来的对比，为什么这么做可以查看书\n",
    "\n",
    "    :param train_mat:  type is ndarray\n",
    "\n",
    "                    总的输入文本，大致是 [[0,1,0,1], [], []]\n",
    "\n",
    "    :param train_category: 文件对应的类别分类， [0, 1, 0],\n",
    "\n",
    "                            列表的长度应该等于上面那个输入文本的长度\n",
    "\n",
    "    :return: \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    train_doc_num = len(train_mat)\n",
    "\n",
    "    words_num = len(train_mat[0])\n",
    "\n",
    "    # 因为侮辱性的被标记为了1， 所以只要把他们相加就可以得到侮辱性的有多少\n",
    "\n",
    "    # 侮辱性文件的出现概率，即train_category中所有的1的个数，\n",
    "\n",
    "    # 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率\n",
    "\n",
    "    pos_abusive = np.sum(train_category) / train_doc_num\n",
    "\n",
    "    # 单词出现的次数\n",
    "\n",
    "    # 原版，变成ones是修改版，这是为了防止数字过小溢出\n",
    "\n",
    "    # p0num = np.zeros(words_num)\n",
    "\n",
    "    # p1num = np.zeros(words_num)\n",
    "\n",
    "    p0num = np.ones(words_num)\n",
    "\n",
    "    p1num = np.ones(words_num)\n",
    "\n",
    "    # 整个数据集单词出现的次数（原来是0，后面改成2了）\n",
    "\n",
    "    p0num_all = 2.0\n",
    "\n",
    "    p1num_all = 2.0\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(train_doc_num):\n",
    "\n",
    "        # 遍历所有的文件，如果是侮辱性文件，就计算此侮辱性文件中出现的侮辱性单词的个数\n",
    "\n",
    "        if train_category[i] == 1:\n",
    "\n",
    "            p1num += train_mat[i]\n",
    "\n",
    "            p1num_all += np.sum(train_mat[i])\n",
    "\n",
    "        else:\n",
    "\n",
    "            p0num += train_mat[i]\n",
    "\n",
    "            p0num_all += np.sum(train_mat[i])\n",
    "\n",
    "    # 后面改成取 log 函数\n",
    "\n",
    "    p1vec = np.log(p1num / p1num_all)\n",
    "\n",
    "    p0vec = np.log(p0num / p0num_all)\n",
    "\n",
    "    return p0vec, p1vec, pos_abusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2classify, p0vec, p1vec, p_class1):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    使用算法：\n",
    "\n",
    "        # 将乘法转换为加法\n",
    "\n",
    "        乘法：P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)\n",
    "\n",
    "        加法：P(F1|C)*P(F2|C)....P(Fn|C)P(C) -> log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "\n",
    "    :param vec2classify: 待测数据[0,1,1,1,1...]，即要分类的向量\n",
    "\n",
    "    :param p0vec: 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表\n",
    "\n",
    "    :param p1vec: 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表\n",
    "\n",
    "    :param p_class1: 类别1，侮辱性文件的出现概率\n",
    "\n",
    "    :return: 类别1 or 0\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # 计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "\n",
    "    # 使用 NumPy 数组来计算两个向量相乘的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘，然后将第2个元素相乘，以此类推。\n",
    "\n",
    "    # 我的理解是：这里的 vec2Classify * p1Vec 的意思就是将每个词与其对应的概率相关联起来\n",
    "\n",
    "    # 可以理解为 1.单词在词汇表中的条件下，文件是good 类别的概率 也可以理解为 2.在整个空间下，文件既在词汇表中又是good类别的概率\n",
    "    print(\"vec2classify=%s\" %vec2classify)\n",
    "    print(\"p0vec=%s\" %p0vec)\n",
    "    print(\"p1vec=%s\" %p1vec)\n",
    "    print(\"p_class1=%s\" %p_class1)\n",
    "    p1 = np.sum(vec2classify * p1vec) + np.log(p_class1)\n",
    "    print(\"p0=%s\" %p1)\n",
    "    p0 = np.sum(vec2classify * p0vec) + np.log(1 - p_class1)\n",
    "    print(\"p1=%s\" %p0)\n",
    "    if p1 > p0:\n",
    "\n",
    "        return 1\n",
    "\n",
    "    else:\n",
    "\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    测试朴素贝叶斯算法\n",
    "\n",
    "    :return: no return \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. 加载数据集\n",
    "\n",
    "    list_post, list_classes = loadDataSet()\n",
    "    print(\"list_post=%s\" %list_post)\n",
    "    print(\"list_classes=%s\" %list_classes)\n",
    "    # 2. 创建单词集合\n",
    "\n",
    "    vocab_list = createVocabList(list_post)\n",
    "    print(\"vocab_list=%s\" %vocab_list)\n",
    "\n",
    "\n",
    "    # 3. 计算单词是否出现并创建数据矩阵\n",
    "\n",
    "    train_mat = []\n",
    "\n",
    "    for post_in in list_post:\n",
    "\n",
    "        train_mat.append(\n",
    "\n",
    "            # 返回m*len(vocab_list)的矩阵， 记录的都是0，1信息\n",
    "\n",
    "            # 其实就是那个东西的句子向量（就是data_set里面每一行,也不算句子吧)\n",
    "\n",
    "            setOfWords2Vec(vocab_list, post_in)\n",
    "\n",
    "        )\n",
    "    print(\"train_mat=%s\" %train_mat)\n",
    "    # 4. 训练数据\n",
    "\n",
    "    p0v, p1v, p_abusive = trainNB(np.array(train_mat), np.array(list_classes))\n",
    "    # 5. 测试数据\n",
    "\n",
    "    test_one = ['love', 'my', 'dalmation']\n",
    "    test_one_doc = np.array(setOfWords2Vec(vocab_list, test_one))\n",
    "    print(\"test_one_doc=%s\" %test_one_doc)\n",
    "    print('the result is: {}'.format(classifyNB(test_one_doc, p0v, p1v, p_abusive)))\n",
    "\n",
    "    test_two = ['stupid', 'garbage']\n",
    "\n",
    "    test_two_doc = np.array(setOfWords2Vec(vocab_list, test_two))\n",
    "\n",
    "    print('the result is: {}'.format(classifyNB(test_two_doc, p0v, p1v, p_abusive)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_post=[['my', 'my', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
      "list_classes=[0, 1, 0, 1, 0, 1]\n",
      "vocab_list=['dalmation', 'love', 'posting', 'help', 'mr', 'maybe', 'worthless', 'please', 'my', 'buying', 'dog', 'cute', 'quit', 'so', 'ate', 'park', 'not', 'stupid', 'problems', 'take', 'has', 'how', 'I', 'garbage', 'to', 'licks', 'food', 'flea', 'steak', 'stop', 'is', 'him']\n",
      "train_mat=[[0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]\n",
      "p0v=[-2.52572864 -2.52572864 -3.21887582 -2.52572864 -2.52572864 -3.21887582\n",
      " -3.21887582 -2.52572864 -1.83258146 -3.21887582 -3.21887582 -2.52572864\n",
      " -3.21887582 -2.52572864 -2.52572864 -3.21887582 -3.21887582 -3.21887582\n",
      " -2.52572864 -3.21887582 -2.52572864 -2.52572864 -2.52572864 -3.21887582\n",
      " -2.52572864 -2.52572864 -3.21887582 -2.52572864 -2.52572864 -2.52572864\n",
      " -2.52572864 -2.12026354]\n",
      "p1v=[-3.04452244 -3.04452244 -2.35137526 -3.04452244 -3.04452244 -2.35137526\n",
      " -1.94591015 -3.04452244 -3.04452244 -2.35137526 -1.94591015 -3.04452244\n",
      " -2.35137526 -3.04452244 -3.04452244 -2.35137526 -2.35137526 -1.65822808\n",
      " -3.04452244 -2.35137526 -3.04452244 -3.04452244 -3.04452244 -2.35137526\n",
      " -2.35137526 -3.04452244 -2.35137526 -3.04452244 -3.04452244 -2.35137526\n",
      " -3.04452244 -2.35137526]\n",
      "p_abusive=0.5\n",
      "test_one_doc=[1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "vec2classify=[1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "p0vec=[-2.52572864 -2.52572864 -3.21887582 -2.52572864 -2.52572864 -3.21887582\n",
      " -3.21887582 -2.52572864 -1.83258146 -3.21887582 -3.21887582 -2.52572864\n",
      " -3.21887582 -2.52572864 -2.52572864 -3.21887582 -3.21887582 -3.21887582\n",
      " -2.52572864 -3.21887582 -2.52572864 -2.52572864 -2.52572864 -3.21887582\n",
      " -2.52572864 -2.52572864 -3.21887582 -2.52572864 -2.52572864 -2.52572864\n",
      " -2.52572864 -2.12026354]\n",
      "p1vec=[-3.04452244 -3.04452244 -2.35137526 -3.04452244 -3.04452244 -2.35137526\n",
      " -1.94591015 -3.04452244 -3.04452244 -2.35137526 -1.94591015 -3.04452244\n",
      " -2.35137526 -3.04452244 -3.04452244 -2.35137526 -2.35137526 -1.65822808\n",
      " -3.04452244 -2.35137526 -3.04452244 -3.04452244 -3.04452244 -2.35137526\n",
      " -2.35137526 -3.04452244 -2.35137526 -3.04452244 -3.04452244 -2.35137526\n",
      " -3.04452244 -2.35137526]\n",
      "p_class1=0.5\n",
      "p0=-9.826714493730215\n",
      "p1=-7.577185932924767\n",
      "the result is: 0\n",
      "vec2classify=[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "p0vec=[-2.52572864 -2.52572864 -3.21887582 -2.52572864 -2.52572864 -3.21887582\n",
      " -3.21887582 -2.52572864 -1.83258146 -3.21887582 -3.21887582 -2.52572864\n",
      " -3.21887582 -2.52572864 -2.52572864 -3.21887582 -3.21887582 -3.21887582\n",
      " -2.52572864 -3.21887582 -2.52572864 -2.52572864 -2.52572864 -3.21887582\n",
      " -2.52572864 -2.52572864 -3.21887582 -2.52572864 -2.52572864 -2.52572864\n",
      " -2.52572864 -2.12026354]\n",
      "p1vec=[-3.04452244 -3.04452244 -2.35137526 -3.04452244 -3.04452244 -2.35137526\n",
      " -1.94591015 -3.04452244 -3.04452244 -2.35137526 -1.94591015 -3.04452244\n",
      " -2.35137526 -3.04452244 -3.04452244 -2.35137526 -2.35137526 -1.65822808\n",
      " -3.04452244 -2.35137526 -3.04452244 -3.04452244 -3.04452244 -2.35137526\n",
      " -2.35137526 -3.04452244 -2.35137526 -3.04452244 -3.04452244 -2.35137526\n",
      " -3.04452244 -2.35137526]\n",
      "p_class1=0.5\n",
      "p0=-4.702750514326955\n",
      "p1=-7.1308988302963465\n",
      "the result is: 1\n"
     ]
    }
   ],
   "source": [
    " testingNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BagOfWords2VecMN(vocabList, inputSet):\n",
    "    \"\"\"\n",
    "    遍历查看该单词是否出现，出现该单词则将该单词置1\n",
    "    :param vocabList: 所有单词集合列表\n",
    "    :param inputSet: 输入数据集\n",
    "    :return: 匹配列表[0,1,0,1...]，其中 1与0 表示词汇表中的单词是否出现在输入的数据集中\n",
    "    \"\"\"\n",
    "    # 创建一个和词汇表等长的向量，并将其元素都设置为0\n",
    "    returnVec = [0] * len(vocabList)# [0,0......]\n",
    "    # 遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
